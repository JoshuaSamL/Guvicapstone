{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#feauture engineering\n",
    "from sklearn.preprocessing import LabelEncoder, MinMaxScaler\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_excel(\"J:\\Studies\\GUVI\\projects\\capstone\\Zomato\\zomato.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'columns_to_drop' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m df1 \u001b[38;5;241m=\u001b[39m df\u001b[38;5;241m.\u001b[39mdrop(columns\u001b[38;5;241m=\u001b[39m\u001b[43mcolumns_to_drop\u001b[49m, errors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mignore\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      2\u001b[0m df2 \u001b[38;5;241m=\u001b[39m df1\u001b[38;5;241m.\u001b[39mdrop(columns\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mAverage_Cost_For_Two\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m      3\u001b[0m df2\n",
      "\u001b[1;31mNameError\u001b[0m: name 'columns_to_drop' is not defined"
     ]
    }
   ],
   "source": [
    "df1 = df.drop(columns=columns_to_drop, errors='ignore')\n",
    "df2 = df1.drop(columns=['Average_Cost_For_Two'])\n",
    "df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature Engineering Complete:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Name</th>\n",
       "      <th>Cuisines</th>\n",
       "      <th>Average_Cost_For_Two</th>\n",
       "      <th>Price_Range</th>\n",
       "      <th>Has_Online_Delivery</th>\n",
       "      <th>Is_Delivering_Now</th>\n",
       "      <th>Menu_URL</th>\n",
       "      <th>Booking_URL</th>\n",
       "      <th>Featured_Image</th>\n",
       "      <th>Latitude</th>\n",
       "      <th>Longitude</th>\n",
       "      <th>Address</th>\n",
       "      <th>City</th>\n",
       "      <th>Aggregate_Rating</th>\n",
       "      <th>Rating_Text</th>\n",
       "      <th>Votes</th>\n",
       "      <th>Res_ID</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Hauz Khas Social</td>\n",
       "      <td>628</td>\n",
       "      <td>0.002000</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>https://www.zomato.com/HauzKhasSocial/menu?utm...</td>\n",
       "      <td>https://www.zomato.com/HauzKhasSocial/book?utm...</td>\n",
       "      <td>https://b.zmtcdn.com/data/pictures/2/308322/cf...</td>\n",
       "      <td>28.554285</td>\n",
       "      <td>77.194471</td>\n",
       "      <td>9-A &amp; 12, Hauz Khas Village, New Delhi</td>\n",
       "      <td>89</td>\n",
       "      <td>0.877551</td>\n",
       "      <td>5</td>\n",
       "      <td>0.725352</td>\n",
       "      <td>308322</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Qubitos - The Terrace Cafe</td>\n",
       "      <td>1812</td>\n",
       "      <td>0.001875</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>https://www.zomato.com/ncr/qubitos-the-terrace...</td>\n",
       "      <td>https://www.zomato.com/ncr/qubitos-the-terrace...</td>\n",
       "      <td>https://b.zmtcdn.com/data/pictures/7/18037817/...</td>\n",
       "      <td>28.647133</td>\n",
       "      <td>77.117701</td>\n",
       "      <td>C-7, Vishal Enclave, Opposite Metro Pillar 417...</td>\n",
       "      <td>89</td>\n",
       "      <td>0.918367</td>\n",
       "      <td>1</td>\n",
       "      <td>0.071154</td>\n",
       "      <td>18037817</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>The Hudson Cafe</td>\n",
       "      <td>428</td>\n",
       "      <td>0.001063</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>https://www.zomato.com/ncr/the-hudson-cafe-del...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>https://b.zmtcdn.com/data/pictures/5/312345/03...</td>\n",
       "      <td>28.694947</td>\n",
       "      <td>77.204317</td>\n",
       "      <td>2524, 1st Floor, Hudson Lane, Delhi University...</td>\n",
       "      <td>89</td>\n",
       "      <td>0.897959</td>\n",
       "      <td>5</td>\n",
       "      <td>0.140571</td>\n",
       "      <td>312345</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Summer House Cafe</td>\n",
       "      <td>1046</td>\n",
       "      <td>0.002313</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>https://www.zomato.com/ncr/summer-house-cafe-h...</td>\n",
       "      <td>https://www.zomato.com/ncr/summer-house-cafe-h...</td>\n",
       "      <td>https://b.zmtcdn.com/data/pictures/0/307490/e0...</td>\n",
       "      <td>28.552520</td>\n",
       "      <td>77.203809</td>\n",
       "      <td>1st Floor, DDA Shopping Complex, Aurobindo Pla...</td>\n",
       "      <td>89</td>\n",
       "      <td>0.836735</td>\n",
       "      <td>5</td>\n",
       "      <td>0.166728</td>\n",
       "      <td>307490</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>38 Barracks</td>\n",
       "      <td>1475</td>\n",
       "      <td>0.002000</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>https://www.zomato.com/ncr/38-barracks-connaug...</td>\n",
       "      <td>https://www.zomato.com/ncr/38-barracks-connaug...</td>\n",
       "      <td>https://b.zmtcdn.com/data/pictures/7/18241537/...</td>\n",
       "      <td>28.633025</td>\n",
       "      <td>77.222858</td>\n",
       "      <td>M-38, Outer Circle, Connaught Place, New Delhi</td>\n",
       "      <td>89</td>\n",
       "      <td>0.897959</td>\n",
       "      <td>5</td>\n",
       "      <td>0.076825</td>\n",
       "      <td>18241537</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         Name  Cuisines  Average_Cost_For_Two  Price_Range  \\\n",
       "0            Hauz Khas Social       628              0.002000            3   \n",
       "1  Qubitos - The Terrace Cafe      1812              0.001875            3   \n",
       "2             The Hudson Cafe       428              0.001063            2   \n",
       "3           Summer House Cafe      1046              0.002313            3   \n",
       "4                 38 Barracks      1475              0.002000            3   \n",
       "\n",
       "   Has_Online_Delivery  Is_Delivering_Now  \\\n",
       "0                    1                  0   \n",
       "1                    0                  0   \n",
       "2                    1                  0   \n",
       "3                    0                  0   \n",
       "4                    0                  0   \n",
       "\n",
       "                                            Menu_URL  \\\n",
       "0  https://www.zomato.com/HauzKhasSocial/menu?utm...   \n",
       "1  https://www.zomato.com/ncr/qubitos-the-terrace...   \n",
       "2  https://www.zomato.com/ncr/the-hudson-cafe-del...   \n",
       "3  https://www.zomato.com/ncr/summer-house-cafe-h...   \n",
       "4  https://www.zomato.com/ncr/38-barracks-connaug...   \n",
       "\n",
       "                                         Booking_URL  \\\n",
       "0  https://www.zomato.com/HauzKhasSocial/book?utm...   \n",
       "1  https://www.zomato.com/ncr/qubitos-the-terrace...   \n",
       "2                                                NaN   \n",
       "3  https://www.zomato.com/ncr/summer-house-cafe-h...   \n",
       "4  https://www.zomato.com/ncr/38-barracks-connaug...   \n",
       "\n",
       "                                      Featured_Image   Latitude  Longitude  \\\n",
       "0  https://b.zmtcdn.com/data/pictures/2/308322/cf...  28.554285  77.194471   \n",
       "1  https://b.zmtcdn.com/data/pictures/7/18037817/...  28.647133  77.117701   \n",
       "2  https://b.zmtcdn.com/data/pictures/5/312345/03...  28.694947  77.204317   \n",
       "3  https://b.zmtcdn.com/data/pictures/0/307490/e0...  28.552520  77.203809   \n",
       "4  https://b.zmtcdn.com/data/pictures/7/18241537/...  28.633025  77.222858   \n",
       "\n",
       "                                             Address  City  Aggregate_Rating  \\\n",
       "0             9-A & 12, Hauz Khas Village, New Delhi    89          0.877551   \n",
       "1  C-7, Vishal Enclave, Opposite Metro Pillar 417...    89          0.918367   \n",
       "2  2524, 1st Floor, Hudson Lane, Delhi University...    89          0.897959   \n",
       "3  1st Floor, DDA Shopping Complex, Aurobindo Pla...    89          0.836735   \n",
       "4     M-38, Outer Circle, Connaught Place, New Delhi    89          0.897959   \n",
       "\n",
       "   Rating_Text     Votes    Res_ID  \n",
       "0            5  0.725352    308322  \n",
       "1            1  0.071154  18037817  \n",
       "2            5  0.140571    312345  \n",
       "3            5  0.166728    307490  \n",
       "4            5  0.076825  18241537  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Encode categorical features\n",
    "label_encoders = {}\n",
    "categorical_columns = ['Cuisines', 'City', 'Rating_Text']\n",
    "for col in categorical_columns:\n",
    "    le = LabelEncoder()\n",
    "    df[col] = le.fit_transform(df[col].astype(str))\n",
    "    label_encoders[col] = le  # Save encoder for later use\n",
    "\n",
    "# Normalize numerical features\n",
    "scaler = MinMaxScaler()\n",
    "numerical_columns = ['Average_Cost_For_Two', 'Votes', 'Aggregate_Rating']\n",
    "df[numerical_columns] = scaler.fit_transform(df[numerical_columns])\n",
    "\n",
    "print(\"Feature Engineering Complete:\")\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Package(s) not found: scikitlearn\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model development\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      "Training model...\n",
      "\n",
      "Model Evaluation Metrics:\n",
      "RMSE: 0.0095\n",
      "MAE: 0.0003\n",
      "R² Score: 0.8414\n",
      "\n",
      "Saving files...\n",
      "Files saved successfully!\n",
      "- cleaned_data.pkl\n",
      "- restaurant_cost_prediction_model.pkl\n",
      "- label_encoders.pkl\n",
      "- input_scaler.pkl\n",
      "- target_scaler.pkl\n",
      "- feature_names.pkl\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.preprocessing import LabelEncoder, MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "import pickle\n",
    "\n",
    "# Load the data\n",
    "print(\"Loading data...\")\n",
    "df = pd.read_excel(\"zomato.xlsx\")\n",
    "\n",
    "# Make a copy of the DataFrame\n",
    "df_cleaned = df.copy()\n",
    "\n",
    "# Identify columns to keep\n",
    "columns_to_keep = [\n",
    "    'Name',  # Keep restaurant name for display\n",
    "    'Cuisines', \n",
    "    'Price_Range', \n",
    "    'Has_Online_Delivery',\n",
    "    'Is_Delivering_Now', \n",
    "    'City', \n",
    "    'Aggregate_Rating',\n",
    "    'Rating_Text',\n",
    "    'Votes',\n",
    "    'Average_Cost_For_Two'\n",
    "]\n",
    "\n",
    "# Keep only the columns we need\n",
    "df_cleaned = df_cleaned[columns_to_keep]\n",
    "\n",
    "# Handle missing values if any\n",
    "df_cleaned = df_cleaned.dropna()\n",
    "\n",
    "# Create copy for encoding\n",
    "df_encoded = df_cleaned.copy()\n",
    "\n",
    "# Initialize scalers\n",
    "input_scaler = MinMaxScaler()\n",
    "target_scaler = MinMaxScaler()\n",
    "\n",
    "# Encode categorical columns\n",
    "categorical_columns = ['Cuisines', 'City', 'Rating_Text']\n",
    "label_encoders = {}\n",
    "\n",
    "for col in categorical_columns:\n",
    "    label_encoders[col] = LabelEncoder()\n",
    "    df_encoded[col] = label_encoders[col].fit_transform(df_encoded[col].astype(str))\n",
    "\n",
    "# Scale input numerical features\n",
    "numerical_features = ['Votes', 'Aggregate_Rating']\n",
    "df_encoded[numerical_features] = input_scaler.fit_transform(df_encoded[numerical_features])\n",
    "\n",
    "# Scale target variable\n",
    "df_encoded['Average_Cost_For_Two'] = target_scaler.fit_transform(df_encoded[['Average_Cost_For_Two']])\n",
    "\n",
    "# Separate features and target\n",
    "feature_columns = ['Cuisines', 'Price_Range', 'Has_Online_Delivery', 'Is_Delivering_Now', \n",
    "                  'City', 'Aggregate_Rating', 'Rating_Text', 'Votes']\n",
    "X = df_encoded[feature_columns]\n",
    "y = df_encoded['Average_Cost_For_Two']\n",
    "\n",
    "# Split the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train the model\n",
    "print(\"Training model...\")\n",
    "model = RandomForestRegressor(\n",
    "    n_estimators=100,\n",
    "    max_depth=15,\n",
    "    min_samples_split=5,\n",
    "    min_samples_leaf=2,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on test set\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Calculate metrics\n",
    "rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "print(\"\\nModel Evaluation Metrics:\")\n",
    "print(f\"RMSE: {rmse:.4f}\")\n",
    "print(f\"MAE: {mae:.4f}\")\n",
    "print(f\"R² Score: {r2:.4f}\")\n",
    "\n",
    "# Save all necessary files\n",
    "print(\"\\nSaving files...\")\n",
    "\n",
    "# Save the original dataframe for reference\n",
    "df_cleaned.to_pickle('cleaned_data.pkl')\n",
    "\n",
    "with open('restaurant_cost_prediction_model.pkl', 'wb') as f:\n",
    "    pickle.dump(model, f)\n",
    "\n",
    "with open('label_encoders.pkl', 'wb') as f:\n",
    "    pickle.dump(label_encoders, f)\n",
    "\n",
    "with open('input_scaler.pkl', 'wb') as f:\n",
    "    pickle.dump(input_scaler, f)\n",
    "\n",
    "with open('target_scaler.pkl', 'wb') as f:\n",
    "    pickle.dump(target_scaler, f)\n",
    "\n",
    "with open('feature_names.pkl', 'wb') as f:\n",
    "    pickle.dump(feature_columns, f)\n",
    "\n",
    "print(\"Files saved successfully!\")\n",
    "print(\"- cleaned_data.pkl\")\n",
    "print(\"- restaurant_cost_prediction_model.pkl\")\n",
    "print(\"- label_encoders.pkl\")\n",
    "print(\"- input_scaler.pkl\")\n",
    "print(\"- target_scaler.pkl\")\n",
    "print(\"- feature_names.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Model Evaluation Metrics:\n",
      "RMSE: ₹5844.30\n",
      "MAE: ₹177.39\n",
      "R² Score: 0.9058\n",
      "\n",
      "Feature Importance:\n",
      "               feature    importance\n",
      "4                 City  9.189104e-01\n",
      "0              Cuisine  3.369921e-02\n",
      "7                Votes  2.140749e-02\n",
      "5     Aggregate_Rating  2.117964e-02\n",
      "1          Price_Range  3.147533e-03\n",
      "6          Rating_Text  1.634413e-03\n",
      "2  Has_Online_Delivery  2.125245e-05\n",
      "3    Is_Delivering_Now  5.372766e-08\n",
      "\n",
      "Saved files:\n",
      "- restaurant_cost_prediction_model.pkl\n",
      "- label_encoders.pkl\n",
      "- input_scaler.pkl\n",
      "- target_scaler.pkl\n",
      "- feature_names.pkl\n",
      "- expanded_dataset.pkl\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.preprocessing import LabelEncoder, MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "import pickle\n",
    "\n",
    "# Load the data\n",
    "df = pd.read_excel(\"zomato.xlsx\")\n",
    "\n",
    "# Make a copy of the DataFrame\n",
    "df_cleaned = df.copy()\n",
    "\n",
    "# Split cuisines and create expanded DataFrame\n",
    "def expand_cuisines(df):\n",
    "    # Create rows for each cuisine\n",
    "    expanded_rows = []\n",
    "    for _, row in df.iterrows():\n",
    "        cuisines = str(row['Cuisines']).split(',')\n",
    "        for cuisine in cuisines:\n",
    "            new_row = row.copy()\n",
    "            new_row['Cuisine'] = cuisine.strip()  # Remove whitespace\n",
    "            expanded_rows.append(new_row)\n",
    "    \n",
    "    return pd.DataFrame(expanded_rows)\n",
    "\n",
    "# Expand the DataFrame\n",
    "df_expanded = expand_cuisines(df_cleaned)\n",
    "\n",
    "# Remove the original Cuisines column and rename the new one\n",
    "df_expanded = df_expanded.drop('Cuisines', axis=1)\n",
    "\n",
    "# Identify columns to keep\n",
    "columns_to_keep = [\n",
    "    'Cuisine',  # New column name\n",
    "    'Price_Range', \n",
    "    'Has_Online_Delivery',\n",
    "    'Is_Delivering_Now', \n",
    "    'City', \n",
    "    'Aggregate_Rating',\n",
    "    'Rating_Text',\n",
    "    'Votes',\n",
    "    'Average_Cost_For_Two'\n",
    "]\n",
    "\n",
    "# Keep only the columns we need\n",
    "df_expanded = df_expanded[columns_to_keep]\n",
    "\n",
    "# Handle missing values if any\n",
    "df_expanded = df_expanded.dropna()\n",
    "\n",
    "# Initialize scalers\n",
    "input_scaler = MinMaxScaler()\n",
    "target_scaler = MinMaxScaler()\n",
    "\n",
    "# Encode categorical columns\n",
    "categorical_columns = ['Cuisine', 'City', 'Rating_Text']\n",
    "label_encoders = {}\n",
    "\n",
    "for col in categorical_columns:\n",
    "    label_encoders[col] = LabelEncoder()\n",
    "    df_expanded[col] = label_encoders[col].fit_transform(df_expanded[col].astype(str))\n",
    "\n",
    "# Scale input numerical features\n",
    "numerical_features = ['Votes', 'Aggregate_Rating']\n",
    "df_expanded[numerical_features] = input_scaler.fit_transform(df_expanded[numerical_features])\n",
    "\n",
    "# Scale target variable\n",
    "df_expanded['Average_Cost_For_Two'] = target_scaler.fit_transform(df_expanded[['Average_Cost_For_Two']])\n",
    "\n",
    "# Separate features and target\n",
    "X = df_expanded.drop('Average_Cost_For_Two', axis=1)\n",
    "y = df_expanded['Average_Cost_For_Two']\n",
    "\n",
    "# Split the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train the model\n",
    "model = RandomForestRegressor(\n",
    "    n_estimators=100,\n",
    "    max_depth=15,\n",
    "    min_samples_split=5,\n",
    "    min_samples_leaf=2,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Transform predictions back to original scale for metrics\n",
    "y_test_original = target_scaler.inverse_transform(y_test.values.reshape(-1, 1))\n",
    "y_pred_original = target_scaler.inverse_transform(y_pred.reshape(-1, 1))\n",
    "\n",
    "# Calculate metrics\n",
    "rmse = np.sqrt(mean_squared_error(y_test_original, y_pred_original))\n",
    "mae = mean_absolute_error(y_test_original, y_pred_original)\n",
    "r2 = r2_score(y_test_original, y_pred_original)\n",
    "\n",
    "print(\"\\nModel Evaluation Metrics:\")\n",
    "print(f\"RMSE: ₹{rmse:.2f}\")\n",
    "print(f\"MAE: ₹{mae:.2f}\")\n",
    "print(f\"R² Score: {r2:.4f}\")\n",
    "\n",
    "# Feature importance\n",
    "feature_importance = pd.DataFrame({\n",
    "    'feature': X.columns,\n",
    "    'importance': model.feature_importances_\n",
    "})\n",
    "print(\"\\nFeature Importance:\")\n",
    "print(feature_importance.sort_values('importance', ascending=False))\n",
    "\n",
    "# Save all necessary files\n",
    "with open('restaurant_cost_prediction_model.pkl', 'wb') as f:\n",
    "    pickle.dump(model, f)\n",
    "\n",
    "with open('label_encoders.pkl', 'wb') as f:\n",
    "    pickle.dump(label_encoders, f)\n",
    "\n",
    "with open('input_scaler.pkl', 'wb') as f:\n",
    "    pickle.dump(input_scaler, f)\n",
    "\n",
    "with open('target_scaler.pkl', 'wb') as f:\n",
    "    pickle.dump(target_scaler, f)\n",
    "\n",
    "with open('feature_names.pkl', 'wb') as f:\n",
    "    pickle.dump(list(X.columns), f)\n",
    "\n",
    "# Also save the original DataFrame for reference\n",
    "df_expanded.to_pickle('expanded_dataset.pkl')\n",
    "\n",
    "print(\"\\nSaved files:\")\n",
    "print(\"- restaurant_cost_prediction_model.pkl\")\n",
    "print(\"- label_encoders.pkl\")\n",
    "print(\"- input_scaler.pkl\")\n",
    "print(\"- target_scaler.pkl\")\n",
    "print(\"- feature_names.pkl\")\n",
    "print(\"- expanded_dataset.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Evaluation:\n",
      "RMSE: 0.0091\n",
      "MAE: 0.0003\n"
     ]
    }
   ],
   "source": [
    "# Make a copy of the DataFrame\n",
    "df_cleaned = df.copy()\n",
    "\n",
    "# Encode categorical columns\n",
    "categorical_columns = ['Cuisines', 'City', 'Rating_Text']  # Add other relevant categorical columns if needed\n",
    "label_encoders = {}\n",
    "\n",
    "for col in categorical_columns:\n",
    "    le = LabelEncoder()\n",
    "    df_cleaned[col] = le.fit_transform(df_cleaned[col].astype(str))\n",
    "    label_encoders[col] = le  # Save encoders for later use\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "numerical_columns = ['Average_Cost_For_Two', 'Votes', 'Aggregate_Rating']\n",
    "df[numerical_columns] = scaler.fit_transform(df[numerical_columns])\n",
    "\n",
    "# Drop irrelevant columns\n",
    "columns_to_drop = ['Name', 'Menu_URL', 'Booking_URL', 'Featured_Image', 'Address', 'Latitude', 'Longitude', 'Res_ID']\n",
    "df_cleaned = df_cleaned.drop(columns=columns_to_drop, errors='ignore')\n",
    "\n",
    "# Define features and target variable\n",
    "X = df_cleaned.drop(columns=['Average_Cost_For_Two'])  # Drop target variable\n",
    "y = df_cleaned['Average_Cost_For_Two']  # Target variable\n",
    "\n",
    "# Split the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train a Random Forest model\n",
    "model = RandomForestRegressor(random_state=42, n_estimators=100)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate the model\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Calculate RMSE\n",
    "mse = mean_squared_error(y_test, y_pred)  # Default output is squared\n",
    "rmse = np.sqrt(mse)  # Take the square root for RMSE\n",
    "\n",
    "# Calculate MAE\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "\n",
    "print(f\"Model Evaluation:\\nRMSE: {rmse:.4f}\\nMAE: {mae:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Evaluation:\n",
      "RMSE: 0.0091\n",
      "MAE: 0.0003\n",
      "R² Score: 0.8577\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "import numpy as np\n",
    "\n",
    "# Calculate RMSE\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "rmse = np.sqrt(mse)\n",
    "\n",
    "# Calculate MAE\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "\n",
    "# Calculate R² (coefficient of determination)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "# Print evaluation metrics\n",
    "print(f\"Model Evaluation:\")\n",
    "print(f\"RMSE: {rmse:.4f}\")\n",
    "print(f\"MAE: {mae:.4f}\")\n",
    "print(f\"R² Score: {r2:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'4.0'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pickle.format_version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model and encoders saved:\n",
      "Model Path: restaurant_cost_prediction_model.joblib\n",
      "Encoders Path: label_encoders.joblib\n"
     ]
    }
   ],
   "source": [
    "# Save the trained model\n",
    "model_file_path = \"restaurant_cost_prediction_model.joblib\"\n",
    "with open(model_file_path, 'wb') as file:\n",
    "    joblib.dump(model, file)\n",
    "\n",
    "# Save the LabelEncoders\n",
    "encoder_file_path = \"label_encoders.joblib\"\n",
    "with open(encoder_file_path, 'wb') as file:\n",
    "    joblib.dump(label_encoders, file)\n",
    "\n",
    "# Save feature names\n",
    "feature_names = list(X.columns)  # X is your training dataframe\n",
    "with open(\"feature_names.joblib\", \"wb\") as f:\n",
    "    joblib.dump(feature_names, f)\n",
    "\n",
    "scaler_file_path = \"minmax_scaler.joblib\"\n",
    "with open(scaler_file_path, \"wb\") as file:\n",
    "    joblib.dump(scaler, file)\n",
    "\n",
    "print(f\"Model and encoders saved:\\nModel Path: {model_file_path}\\nEncoders Path: {encoder_file_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model and encoders saved:\n",
      "Model Path: restaurant_cost_prediction_model.pkl\n",
      "Encoders Path: label_encoders.pkl\n"
     ]
    }
   ],
   "source": [
    "# Save the trained model\n",
    "model_file_path = \"restaurant_cost_prediction_model.pkl\"\n",
    "with open(model_file_path, 'wb') as file:\n",
    "    pickle.dump(model, file)\n",
    "\n",
    "# Save the LabelEncoders\n",
    "encoder_file_path = \"label_encoders.pkl\"\n",
    "with open(encoder_file_path, 'wb') as file:\n",
    "    pickle.dump(label_encoders, file)\n",
    "\n",
    "# Save feature names\n",
    "feature_names = list(X.columns)  # X is your training dataframe\n",
    "with open(\"feature_names.pkl\", \"wb\") as f:\n",
    "    pickle.dump(feature_names, f)\n",
    "\n",
    "scaler_file_path = \"minmax_scaler.pkl\"\n",
    "with open(scaler_file_path, \"wb\") as file:\n",
    "    pickle.dump(scaler, file)\n",
    "\n",
    "print(f\"Model and encoders saved:\\nModel Path: {model_file_path}\\nEncoders Path: {encoder_file_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unseen value: 'Cafe'. Using fallback: 'Other'.\n",
      "Unseen value: 'Very Good'. Using fallback: 'Good'.\n",
      "Unseen value: 'New Delhi'. Using fallback: 'Other'.\n",
      "The predicted average cost for two is: ₹0.00\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import pickle\n",
    "import numpy as np\n",
    "\n",
    "# Load the model and encoders\n",
    "model_file_path = \"restaurant_cost_prediction_model.pkl\"  # Replace with your file path\n",
    "encoder_file_path = \"label_encoders.pkl\"\n",
    "feature_names_file_path = \"feature_names.pkl\"\n",
    "\n",
    "with open(model_file_path, 'rb') as file:\n",
    "    model = pickle.load(file)\n",
    "\n",
    "with open(encoder_file_path, 'rb') as file:\n",
    "    label_encoders = pickle.load(file)\n",
    "\n",
    "with open(feature_names_file_path, \"rb\") as file:\n",
    "    trained_features = pickle.load(file)\n",
    "\n",
    "# Load dataset for visualization (if needed)\n",
    "data_file_path = \"J:\\\\Studies\\\\GUVI\\\\projects\\\\capstone\\\\Zomato\\\\zomato.xlsx\"  # Replace with your file path\n",
    "df = pd.read_excel(data_file_path)\n",
    "\n",
    "# Input restaurant details\n",
    "cuisine_input = input(\"Enter Cuisine: \")\n",
    "rating_text_input = input(\"Enter Rating Text: \")\n",
    "votes_input = int(input(\"Enter Votes: \"))\n",
    "selected_city = input(\"Enter City: \")\n",
    "\n",
    "# Add fallback categories to LabelEncoder if needed\n",
    "def ensure_fallback_class(label_encoder, fallback_value):\n",
    "    if fallback_value not in label_encoder.classes_:\n",
    "        label_encoder.classes_ = np.append(label_encoder.classes_, fallback_value)\n",
    "\n",
    "# Ensure fallback values are added\n",
    "ensure_fallback_class(label_encoders['Cuisines'], \"Other\")\n",
    "ensure_fallback_class(label_encoders['Rating_Text'], \"Good\")\n",
    "ensure_fallback_class(label_encoders['City'], \"Other\")  # Ensure fallback for City as well\n",
    "\n",
    "# Preprocess cuisine input: Take the first cuisine if it’s a comma-separated string\n",
    "def preprocess_cuisine(cuisine):\n",
    "    if isinstance(cuisine, str) and ',' in cuisine:\n",
    "        return cuisine.split(',')[0].strip()  # Take the first cuisine\n",
    "    return cuisine.strip() if isinstance(cuisine, str) else \"Other\"\n",
    "\n",
    "# Encode with fallback\n",
    "def safe_encode(label_encoder, value, fallback_value):\n",
    "    try:\n",
    "        if value in label_encoder.classes_:\n",
    "            return label_encoder.transform([value])[0]\n",
    "        else:\n",
    "            print(f\"Unseen value: '{value}'. Using fallback: '{fallback_value}'.\")\n",
    "            return label_encoder.transform([fallback_value])[0]\n",
    "    except Exception as e:\n",
    "        print(f\"Encoding error: {e}\")\n",
    "        raise\n",
    "\n",
    "# Preprocess inputs\n",
    "processed_cuisine_input = preprocess_cuisine(cuisine_input)\n",
    "encoded_cuisine = safe_encode(label_encoders['Cuisines'], processed_cuisine_input, \"Other\")\n",
    "encoded_rating_text = safe_encode(label_encoders['Rating_Text'], rating_text_input, \"Good\")\n",
    "encoded_city = safe_encode(label_encoders['City'], selected_city, \"Other\")  # Encode the city\n",
    "\n",
    "# Add all required features for prediction\n",
    "default_values = {\n",
    "    \"Aggregate_Rating\": 3.5,  # Default average rating\n",
    "    \"Has_Online_Delivery\": 0,  # Assuming default is 'No' (0)\n",
    "    \"Is_Delivering_Now\": 0,  # Assuming default is 'No' (0)\n",
    "    \"Price_Range\": 2,  # Assuming mid-range price as default\n",
    "}\n",
    "\n",
    "input_features = {\n",
    "    'Cuisines': encoded_cuisine,\n",
    "    'Price_Range': default_values['Price_Range'],\n",
    "    'Has_Online_Delivery': default_values['Has_Online_Delivery'],\n",
    "    'Is_Delivering_Now': default_values['Is_Delivering_Now'],\n",
    "    'City': encoded_city,  # Encoded city\n",
    "    'Aggregate_Rating': default_values['Aggregate_Rating'],\n",
    "    'Rating_Text': encoded_rating_text,\n",
    "    'Votes': votes_input,\n",
    "}\n",
    "\n",
    "# Convert input dictionary to DataFrame\n",
    "input_df = pd.DataFrame([input_features])\n",
    "\n",
    "# Match the column order used during model training\n",
    "expected_columns = ['Cuisines', 'Price_Range', 'Has_Online_Delivery', \n",
    "                    'Is_Delivering_Now', 'City', 'Aggregate_Rating', \n",
    "                    'Rating_Text', 'Votes']\n",
    "input_df = input_df[expected_columns]\n",
    "\n",
    "# Make prediction\n",
    "try:\n",
    "    prediction = model.predict(input_df)[0]\n",
    "    print(f\"The predicted average cost for two is: ₹{prediction:.2f}\")\n",
    "except Exception as e:\n",
    "    print(f\"Prediction error: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unseen value: 'Continental'. Using fallback: 'Other'.\n",
      "Unseen value: 'Very Good'. Using fallback: 'Good'.\n",
      "Unseen value: 'New Delhi'. Using fallback: 'Other'.\n",
      "Scaler error: The feature names should match those that were passed during fit.\n",
      "Feature names unseen at fit time:\n",
      "- City\n",
      "- Cuisines\n",
      "- Has_Online_Delivery\n",
      "- Is_Delivering_Now\n",
      "- Price_Range\n",
      "- ...\n",
      "Feature names seen at fit time, yet now missing:\n",
      "- Average_Cost_For_Two\n",
      "\n",
      "Input columns: Index(['Cuisines', 'Price_Range', 'Has_Online_Delivery', 'Is_Delivering_Now',\n",
      "       'City', 'Aggregate_Rating', 'Rating_Text', 'Votes'],\n",
      "      dtype='object')\n",
      "Expected columns: ['Cuisines', 'Price_Range', 'Has_Online_Delivery', 'Is_Delivering_Now', 'City', 'Aggregate_Rating', 'Rating_Text', 'Votes']\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "The feature names should match those that were passed during fit.\nFeature names unseen at fit time:\n- City\n- Cuisines\n- Has_Online_Delivery\n- Is_Delivering_Now\n- Price_Range\n- ...\nFeature names seen at fit time, yet now missing:\n- Average_Cost_For_Two\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[27], line 90\u001b[0m\n\u001b[0;32m     88\u001b[0m \u001b[38;5;66;03m# Scale the input features using the loaded MinMaxScaler\u001b[39;00m\n\u001b[0;32m     89\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 90\u001b[0m     scaled_input_df \u001b[38;5;241m=\u001b[39m \u001b[43mscaler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransform\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_df\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     91\u001b[0m     \u001b[38;5;66;03m# Make prediction\u001b[39;00m\n\u001b[0;32m     92\u001b[0m     prediction \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mpredict(scaled_input_df)[\u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[1;32mc:\\Users\\Joshua Sam\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\utils\\_set_output.py:319\u001b[0m, in \u001b[0;36m_wrap_method_output.<locals>.wrapped\u001b[1;34m(self, X, *args, **kwargs)\u001b[0m\n\u001b[0;32m    317\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(f)\n\u001b[0;32m    318\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapped\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m--> 319\u001b[0m     data_to_wrap \u001b[38;5;241m=\u001b[39m f(\u001b[38;5;28mself\u001b[39m, X, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    320\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data_to_wrap, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[0;32m    321\u001b[0m         \u001b[38;5;66;03m# only wrap the first output for cross decomposition\u001b[39;00m\n\u001b[0;32m    322\u001b[0m         return_tuple \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    323\u001b[0m             _wrap_data_with_container(method, data_to_wrap[\u001b[38;5;241m0\u001b[39m], X, \u001b[38;5;28mself\u001b[39m),\n\u001b[0;32m    324\u001b[0m             \u001b[38;5;241m*\u001b[39mdata_to_wrap[\u001b[38;5;241m1\u001b[39m:],\n\u001b[0;32m    325\u001b[0m         )\n",
      "File \u001b[1;32mc:\\Users\\Joshua Sam\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\preprocessing\\_data.py:532\u001b[0m, in \u001b[0;36mMinMaxScaler.transform\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    528\u001b[0m check_is_fitted(\u001b[38;5;28mself\u001b[39m)\n\u001b[0;32m    530\u001b[0m xp, _ \u001b[38;5;241m=\u001b[39m get_namespace(X)\n\u001b[1;32m--> 532\u001b[0m X \u001b[38;5;241m=\u001b[39m \u001b[43mvalidate_data\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    533\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    534\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    535\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    536\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_array_api\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msupported_float_dtypes\u001b[49m\u001b[43m(\u001b[49m\u001b[43mxp\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    537\u001b[0m \u001b[43m    \u001b[49m\u001b[43mforce_writeable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    538\u001b[0m \u001b[43m    \u001b[49m\u001b[43mensure_all_finite\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mallow-nan\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    539\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    540\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    542\u001b[0m X \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscale_\n\u001b[0;32m    543\u001b[0m X \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmin_\n",
      "File \u001b[1;32mc:\\Users\\Joshua Sam\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\utils\\validation.py:2919\u001b[0m, in \u001b[0;36mvalidate_data\u001b[1;34m(_estimator, X, y, reset, validate_separately, skip_check_array, **check_params)\u001b[0m\n\u001b[0;32m   2835\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mvalidate_data\u001b[39m(\n\u001b[0;32m   2836\u001b[0m     _estimator,\n\u001b[0;32m   2837\u001b[0m     \u001b[38;5;241m/\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   2843\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mcheck_params,\n\u001b[0;32m   2844\u001b[0m ):\n\u001b[0;32m   2845\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Validate input data and set or check feature names and counts of the input.\u001b[39;00m\n\u001b[0;32m   2846\u001b[0m \n\u001b[0;32m   2847\u001b[0m \u001b[38;5;124;03m    This helper function should be used in an estimator that requires input\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   2917\u001b[0m \u001b[38;5;124;03m        validated.\u001b[39;00m\n\u001b[0;32m   2918\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 2919\u001b[0m     \u001b[43m_check_feature_names\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_estimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2920\u001b[0m     tags \u001b[38;5;241m=\u001b[39m get_tags(_estimator)\n\u001b[0;32m   2921\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m y \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m tags\u001b[38;5;241m.\u001b[39mtarget_tags\u001b[38;5;241m.\u001b[39mrequired:\n",
      "File \u001b[1;32mc:\\Users\\Joshua Sam\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\utils\\validation.py:2777\u001b[0m, in \u001b[0;36m_check_feature_names\u001b[1;34m(estimator, X, reset)\u001b[0m\n\u001b[0;32m   2774\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m missing_names \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m unexpected_names:\n\u001b[0;32m   2775\u001b[0m     message \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFeature names must be in the same order as they were in fit.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m-> 2777\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(message)\n",
      "\u001b[1;31mValueError\u001b[0m: The feature names should match those that were passed during fit.\nFeature names unseen at fit time:\n- City\n- Cuisines\n- Has_Online_Delivery\n- Is_Delivering_Now\n- Price_Range\n- ...\nFeature names seen at fit time, yet now missing:\n- Average_Cost_For_Two\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import pickle\n",
    "import numpy as np\n",
    "\n",
    "# Load the model, encoders, feature names, and scaler\n",
    "model_file_path = \"restaurant_cost_prediction_model.pkl\"  # Replace with your file path\n",
    "encoder_file_path = \"label_encoders.pkl\"\n",
    "feature_names_file_path = \"feature_names.pkl\"\n",
    "scaler_file_path = \"minmax_scaler.pkl\"  # File path for the saved MinMaxScaler\n",
    "\n",
    "with open(model_file_path, 'rb') as file:\n",
    "    model = pickle.load(file)\n",
    "\n",
    "with open(encoder_file_path, 'rb') as file:\n",
    "    label_encoders = pickle.load(file)\n",
    "\n",
    "with open(feature_names_file_path, \"rb\") as file:\n",
    "    trained_features = pickle.load(file)\n",
    "\n",
    "with open(scaler_file_path, \"rb\") as file:\n",
    "    scaler = pickle.load(file)\n",
    "\n",
    "# Input restaurant details\n",
    "cuisine_input = input(\"Enter Cuisine: \")\n",
    "rating_text_input = input(\"Enter Rating Text: \")\n",
    "votes_input = int(input(\"Enter Votes: \"))\n",
    "selected_city = input(\"Enter City: \")\n",
    "\n",
    "# Add fallback categories to LabelEncoder if needed\n",
    "def ensure_fallback_class(label_encoder, fallback_value):\n",
    "    if fallback_value not in label_encoder.classes_:\n",
    "        label_encoder.classes_ = np.append(label_encoder.classes_, fallback_value)\n",
    "\n",
    "# Ensure fallback values are added\n",
    "ensure_fallback_class(label_encoders['Cuisines'], \"Other\")\n",
    "ensure_fallback_class(label_encoders['Rating_Text'], \"Good\")\n",
    "ensure_fallback_class(label_encoders['City'], \"Other\")  # Ensure fallback for City as well\n",
    "\n",
    "# Preprocess cuisine input: Take the first cuisine if it’s a comma-separated string\n",
    "def preprocess_cuisine(cuisine):\n",
    "    if isinstance(cuisine, str) and ',' in cuisine:\n",
    "        return cuisine.split(',')[0].strip()  # Take the first cuisine\n",
    "    return cuisine.strip() if isinstance(cuisine, str) else \"Other\"\n",
    "\n",
    "# Encode with fallback\n",
    "def safe_encode(label_encoder, value, fallback_value):\n",
    "    try:\n",
    "        if value in label_encoder.classes_:\n",
    "            return label_encoder.transform([value])[0]\n",
    "        else:\n",
    "            print(f\"Unseen value: '{value}'. Using fallback: '{fallback_value}'.\")\n",
    "            return label_encoder.transform([fallback_value])[0]\n",
    "    except Exception as e:\n",
    "        print(f\"Encoding error: {e}\")\n",
    "        raise\n",
    "\n",
    "# Preprocess inputs\n",
    "processed_cuisine_input = preprocess_cuisine(cuisine_input)\n",
    "encoded_cuisine = safe_encode(label_encoders['Cuisines'], processed_cuisine_input, \"Other\")\n",
    "encoded_rating_text = safe_encode(label_encoders['Rating_Text'], rating_text_input, \"Good\")\n",
    "encoded_city = safe_encode(label_encoders['City'], selected_city, \"Other\")  # Encode the city\n",
    "\n",
    "# Add all required features for prediction\n",
    "default_values = {\n",
    "    \"Aggregate_Rating\": 3.5,  # Default average rating\n",
    "    \"Has_Online_Delivery\": 0,  # Assuming default is 'No' (0)\n",
    "    \"Is_Delivering_Now\": 0,  # Assuming default is 'No' (0)\n",
    "    \"Price_Range\": 2,  # Assuming mid-range price as default\n",
    "}\n",
    "\n",
    "input_features = {\n",
    "    'Cuisines': encoded_cuisine,\n",
    "    'Price_Range': default_values['Price_Range'],\n",
    "    'Has_Online_Delivery': default_values['Has_Online_Delivery'],\n",
    "    'Is_Delivering_Now': default_values['Is_Delivering_Now'],\n",
    "    'City': encoded_city,  # Encoded city\n",
    "    'Aggregate_Rating': default_values['Aggregate_Rating'],\n",
    "    'Rating_Text': encoded_rating_text,\n",
    "    'Votes': votes_input,\n",
    "}\n",
    "\n",
    "# Convert input dictionary to DataFrame\n",
    "input_df = pd.DataFrame([input_features])\n",
    "\n",
    "# Match the column order used during scaler fitting and model training\n",
    "input_df = input_df.reindex(columns=trained_features, fill_value=0)\n",
    "\n",
    "# Scale the input features using the loaded MinMaxScaler\n",
    "try:\n",
    "    scaled_input_df = scaler.transform(input_df)\n",
    "    # Make prediction\n",
    "    prediction = model.predict(scaled_input_df)[0]\n",
    "    print(f\"The predicted average cost for two is: ₹{prediction:.2f}\")\n",
    "except ValueError as e:\n",
    "    print(f\"Scaler error: {e}\")\n",
    "    print(f\"Input columns: {input_df.columns}\")\n",
    "    print(f\"Expected columns: {trained_features}\")\n",
    "    raise\n",
    "except Exception as e:\n",
    "    print(f\"Prediction error: {e}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Model Evaluation Metrics:\n",
      "RMSE: ₹7605.98\n",
      "MAE: ₹256.50\n",
      "R² Score: 0.8414\n",
      "\n",
      "Feature Importance:\n",
      "               feature    importance\n",
      "4                 City  9.415408e-01\n",
      "0             Cuisines  3.243716e-02\n",
      "7                Votes  1.265464e-02\n",
      "5     Aggregate_Rating  1.144023e-02\n",
      "1          Price_Range  1.696090e-03\n",
      "6          Rating_Text  1.999268e-04\n",
      "2  Has_Online_Delivery  3.108540e-05\n",
      "3    Is_Delivering_Now  2.704755e-08\n",
      "\n",
      "Saved files:\n",
      "- restaurant_cost_prediction_model.joblib\n",
      "- label_encoders.joblib\n",
      "- input_scaler.joblib\n",
      "- target_scaler.joblib\n",
      "- feature_names.joblib\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.preprocessing import LabelEncoder, MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "import pickle\n",
    "\n",
    "# Load the data\n",
    "df = pd.read_excel(\"zomato.xlsx\")\n",
    "\n",
    "# Make a copy of the DataFrame\n",
    "df_cleaned = df.copy()\n",
    "\n",
    "# Identify columns to keep\n",
    "columns_to_keep = [\n",
    "    'Cuisines', \n",
    "    'Price_Range', \n",
    "    'Has_Online_Delivery',\n",
    "    'Is_Delivering_Now', \n",
    "    'City', \n",
    "    'Aggregate_Rating',\n",
    "    'Rating_Text',\n",
    "    'Votes',\n",
    "    'Average_Cost_For_Two'\n",
    "]\n",
    "\n",
    "# Keep only the columns we need\n",
    "df_cleaned = df_cleaned[columns_to_keep]\n",
    "\n",
    "# Handle missing values if any\n",
    "df_cleaned = df_cleaned.dropna()\n",
    "\n",
    "# Initialize scalers\n",
    "input_scaler = MinMaxScaler()\n",
    "target_scaler = MinMaxScaler()\n",
    "\n",
    "# Encode categorical columns\n",
    "categorical_columns = ['Cuisines', 'City', 'Rating_Text']\n",
    "label_encoders = {}\n",
    "\n",
    "for col in categorical_columns:\n",
    "    label_encoders[col] = LabelEncoder()\n",
    "    df_cleaned[col] = label_encoders[col].fit_transform(df_cleaned[col].astype(str))\n",
    "\n",
    "# Scale input numerical features\n",
    "numerical_features = ['Votes', 'Aggregate_Rating']\n",
    "df_cleaned[numerical_features] = input_scaler.fit_transform(df_cleaned[numerical_features])\n",
    "\n",
    "# Scale target variable\n",
    "df_cleaned['Average_Cost_For_Two'] = target_scaler.fit_transform(df_cleaned[['Average_Cost_For_Two']])\n",
    "\n",
    "# Separate features and target\n",
    "X = df_cleaned.drop('Average_Cost_For_Two', axis=1)\n",
    "y = df_cleaned['Average_Cost_For_Two']\n",
    "\n",
    "# Split the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train the model\n",
    "model = RandomForestRegressor(\n",
    "    n_estimators=100,\n",
    "    max_depth=15,\n",
    "    min_samples_split=5,\n",
    "    min_samples_leaf=2,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Transform predictions back to original scale for metrics\n",
    "y_test_original = target_scaler.inverse_transform(y_test.values.reshape(-1, 1))\n",
    "y_pred_original = target_scaler.inverse_transform(y_pred.reshape(-1, 1))\n",
    "\n",
    "# Calculate metrics\n",
    "rmse = np.sqrt(mean_squared_error(y_test_original, y_pred_original))\n",
    "mae = mean_absolute_error(y_test_original, y_pred_original)\n",
    "r2 = r2_score(y_test_original, y_pred_original)\n",
    "\n",
    "print(\"\\nModel Evaluation Metrics:\")\n",
    "print(f\"RMSE: ₹{rmse:.2f}\")\n",
    "print(f\"MAE: ₹{mae:.2f}\")\n",
    "print(f\"R² Score: {r2:.4f}\")\n",
    "\n",
    "# Feature importance\n",
    "feature_importance = pd.DataFrame({\n",
    "    'feature': X.columns,\n",
    "    'importance': model.feature_importances_\n",
    "})\n",
    "print(\"\\nFeature Importance:\")\n",
    "print(feature_importance.sort_values('importance', ascending=False))\n",
    "\n",
    "# Save all necessary files\n",
    "with open('restaurant_cost_prediction_model.joblib', 'wb') as f:\n",
    "   joblib.dump(model, f)\n",
    "\n",
    "with open('label_encoders.joblib', 'wb') as f:\n",
    "    joblib.dump(label_encoders, f)\n",
    "\n",
    "with open('input_scaler.joblib', 'wb') as f:\n",
    "    joblib.dump(input_scaler, f)\n",
    "\n",
    "with open('target_scaler.joblib', 'wb') as f:\n",
    "    joblib.dump(target_scaler, f)\n",
    "\n",
    "with open('feature_names.joblib', 'wb') as f:\n",
    "    joblib.dump(list(X.columns), f)\n",
    "\n",
    "print(\"\\nSaved files:\")\n",
    "print(\"- restaurant_cost_prediction_model.joblib\")\n",
    "print(\"- label_encoders.joblib\")\n",
    "print(\"- input_scaler.joblib\")\n",
    "print(\"- target_scaler.joblib\")\n",
    "print(\"- feature_names.joblib\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: joblib\n",
      "Version: 1.4.2\n",
      "Summary: Lightweight pipelining with Python functions\n",
      "Home-page: https://joblib.readthedocs.io\n",
      "Author: \n",
      "Author-email: Gael Varoquaux <gael.varoquaux@normalesup.org>\n",
      "License: BSD 3-Clause\n",
      "Location: c:\\users\\joshua sam\\appdata\\local\\programs\\python\\python310\\lib\\site-packages\n",
      "Requires: \n",
      "Required-by: scikit-learn\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip show joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Enter restaurant details:\n",
      "\n",
      "Predicted average cost for two people: ₹869.13\n",
      "Price Category: Mid-range\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "\n",
    "def load_preprocessors():\n",
    "    with open('restaurant_cost_prediction_model.pkl', 'rb') as f:\n",
    "        model = pickle.load(f)\n",
    "    with open('label_encoders.pkl', 'rb') as f:\n",
    "        encoders = pickle.load(f)\n",
    "    with open('input_scaler.pkl', 'rb') as f:\n",
    "        input_scaler = pickle.load(f)\n",
    "    with open('target_scaler.pkl', 'rb') as f:\n",
    "        target_scaler = pickle.load(f)\n",
    "    with open('feature_names.pkl', 'rb') as f:\n",
    "        feature_names = pickle.load(f)\n",
    "    return model, encoders, input_scaler, target_scaler, feature_names\n",
    "\n",
    "def predict_price():\n",
    "    # Load all preprocessors\n",
    "    model, encoders, input_scaler, target_scaler, feature_names = load_preprocessors()\n",
    "    \n",
    "    try:\n",
    "        # Get user input\n",
    "        print(\"\\nEnter restaurant details:\")\n",
    "        cuisine = input(\"Cuisine (e.g., North Indian, Chinese): \")\n",
    "        city = input(\"City (e.g., New Delhi): \")\n",
    "        rating = input(\"Rating Text (Good/Very Good/Excellent): \")\n",
    "        votes = int(input(\"Number of Votes: \"))\n",
    "        \n",
    "        # Create input data dictionary\n",
    "        input_data = {\n",
    "            'Cuisines': cuisine,\n",
    "            'Price_Range': 2,  # Default to mid-range\n",
    "            'Has_Online_Delivery': 0,  # Default values\n",
    "            'Is_Delivering_Now': 0,    # Default values\n",
    "            'City': city,\n",
    "            'Aggregate_Rating': 3.5,  # Default to average rating\n",
    "            'Rating_Text': rating,\n",
    "            'Votes': votes\n",
    "        }\n",
    "        \n",
    "        # Create DataFrame\n",
    "        input_df = pd.DataFrame([input_data])\n",
    "        \n",
    "        # Encode categorical variables\n",
    "        for col in ['Cuisines', 'City', 'Rating_Text']:\n",
    "            try:\n",
    "                input_df[col] = encoders[col].transform(input_df[col].astype(str))\n",
    "            except:\n",
    "                # Handle unknown categories\n",
    "                print(f\"Warning: Unknown {col} category. Using a default value.\")\n",
    "                input_df[col] = encoders[col].transform([encoders[col].classes_[0]])\n",
    "        \n",
    "        # Scale numerical features\n",
    "        numerical_features = ['Votes', 'Aggregate_Rating']\n",
    "        input_df[numerical_features] = input_scaler.transform(input_df[numerical_features])\n",
    "        \n",
    "        # Ensure correct column order\n",
    "        input_df = input_df[feature_names]\n",
    "        \n",
    "        # Make prediction (scaled)\n",
    "        scaled_prediction = model.predict(input_df)[0]\n",
    "        \n",
    "        # Transform prediction back to original scale\n",
    "        prediction = target_scaler.inverse_transform([[scaled_prediction]])[0][0]\n",
    "        \n",
    "        print(f\"\\nPredicted average cost for two people: ₹{prediction:.2f}\")\n",
    "        \n",
    "        # Print additional information\n",
    "        if prediction <= 500:\n",
    "            price_category = \"Budget-friendly\"\n",
    "        elif prediction <= 1500:\n",
    "            price_category = \"Mid-range\"\n",
    "        else:\n",
    "            price_category = \"High-end\"\n",
    "            \n",
    "        print(f\"Price Category: {price_category}\")\n",
    "        \n",
    "    except ValueError as ve:\n",
    "        print(f\"Invalid input: {ve}\")\n",
    "        print(\"Please ensure you enter a valid number for votes.\")\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {str(e)}\")\n",
    "        print(\"Please ensure you have run the training script first and all .pkl files are in the current directory.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    predict_price()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
